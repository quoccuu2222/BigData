from flask import Flask, render_template, request, jsonify, send_file, redirect, url_for
from hdfs import InsecureClient, HdfsError
import os
from io import BytesIO
import traceback
import logging
import base64 
from collections import Counter
import zipfile 
import tempfile 
import findspark

# THI·∫æT L·∫¨P M√îI TR∆Ø·ªúNG B·∫ÆT BU·ªòC TR√äN WINDOWS
# üí° Thi·∫øt l·∫≠p ƒë∆∞·ªùng d·∫´n SPARK_HOME (C·∫ßn thi·∫øt cho PySpark)
os.environ['SPARK_HOME'] = "G:\\Spark\\spark-3.5.7-bin-hadoop3"
print(f"SPARK_HOME ƒë∆∞·ª£c thi·∫øt l·∫≠p: {os.environ['SPARK_HOME']}")

# üí° ƒê√£ c·∫≠p nh·∫≠t HADOOP_HOME theo ƒë∆∞·ªùng d·∫´n c·ªßa ng∆∞·ªùi d√πng
os.environ['HADOOP_HOME'] = "G:\\hadoop\\hadoop-3.3.6"
if not os.path.exists(os.path.join(os.environ['HADOOP_HOME'], 'bin', 'winutils.exe')):
    print("!!! L∆ØU √ù QUAN TR·ªåNG: Kh√¥ng t√¨m th·∫•y winutils.exe t·∫°i ƒë∆∞·ªùng d·∫´n HADOOP_HOME ƒë√£ thi·∫øt l·∫≠p. Vui l√≤ng ki·ªÉm tra l·∫°i ƒë·ªÉ PySpark ho·∫°t ƒë·ªông ·ªïn ƒë·ªãnh.")
else:
    print(f"HADOOP_HOME ƒë∆∞·ª£c thi·∫øt l·∫≠p th√†nh c√¥ng: {os.environ['HADOOP_HOME']}")

# üí° TH√äM: Thi·∫øt l·∫≠p th∆∞ m·ª•c c·∫•u h√¨nh Hadoop
os.environ['HADOOP_CONF_DIR'] = os.path.join(os.environ['HADOOP_HOME'], 'etc', 'hadoop')

# üí° TH√äM: Thi·∫øt l·∫≠p th∆∞ m·ª•c Temp c·ª•c b·ªô 
TEMP_DIR = os.path.join(os.getcwd(), 'spark_temp_dir')
os.makedirs(TEMP_DIR, exist_ok=True)
os.environ['TMPDIR'] = TEMP_DIR
os.environ['TEMP'] = TEMP_DIR

# TH∆Ø VI·ªÜN PYSPARK
try:
    findspark.init() 
except:
    print("C·∫£nh b√°o: Kh√¥ng th·ªÉ kh·ªüi t·∫°o findspark. ƒê·∫£m b·∫£o SPARK_HOME ƒë√£ ƒë∆∞·ª£c ƒë·∫∑t.")
    
os.environ['PYSPARK_PYTHON'] = os.path.join(os.path.dirname(os.sys.executable), 'python.exe')

from pyspark.sql import SparkSession
# üí° ƒê√É C·∫¨P NH·∫¨T: Th√™m DoubleType ƒë·ªÉ √©p ki·ªÉu an to√†n
from pyspark.sql.functions import explode, split, lower, col, avg, round as spark_round, count as spark_count, lit, when, regexp_replace, sum
from pyspark.sql.types import DoubleType, StringType

# TH∆Ø VI·ªÜN TR·ª∞C QUAN H√ìA
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import numpy as np
import builtins

try:
    import joblib
    import xgboost as xgb
    import pandas as pd
    from sklearn.preprocessing import LabelEncoder
except ImportError:
    print("C·∫¢NH B√ÅO: Kh√¥ng th·ªÉ import joblib, xgboost, ho·∫∑c pandas. Ch·ª©c nƒÉng d·ª± ƒëo√°n s·∫Ω kh√¥ng ho·∫°t ƒë·ªông.")
    joblib, xgb, pd, LabelEncoder = None, None, None, None

# C·∫•u h√¨nh log PySpark ƒë·ªÉ tr√°nh spam console
log = logging.getLogger('werkzeug')
log.setLevel(logging.ERROR)

app = Flask(__name__)

# --- C·∫§U H√åNH HDFS ---
HDFS_WEB_URL = 'http://localhost:9870'
HDFS_USER = 'hadoop' # Thay b·∫±ng username c·ªßa b·∫°n
client = InsecureClient(HDFS_WEB_URL, user=HDFS_USER)
HDFS_RPC_URL = 'hdfs://localhost:9000'
global_spark_session = None

# --- KH·ªûI T·∫†O V√Ä T·∫¢I M√î H√åNH D·ª∞ ƒêO√ÅN ---
# Kh·ªüi t·∫°o LABEL_ENCODER m·∫∑c ƒë·ªãnh n·∫øu kh√¥ng t·∫£i ƒë∆∞·ª£c
LABEL_ENCODER = LabelEncoder() if LabelEncoder else None 
if LABEL_ENCODER:
    # ƒê√¢y l√† c√°c nh√£n ƒë∆∞·ª£c hu·∫•n luy·ªán t·ª´ m√¥ h√¨nh c≈©
    LABEL_ENCODER.classes_ = ['CarrierDelay', 'LateAircraftDelay', 'NASDelay', 'SecurityDelay', 'WeatherDelay'] 
    
DELAY_MODEL = None
CAUSE_MODEL = None
MODEL_LOAD_SUCCESS = False

if joblib and xgb and tempfile and client:
    try:
        print("--- ƒêang th·ª≠ t·∫£i M√¥ h√¨nh D·ª± ƒëo√°n t·ª´ HDFS (Th∆∞ m·ª•c g·ªëc) ---")
        
        # S·ª¨A L·ªñI: Ch·ªâ t·∫°o t√™n file t·∫°m th·ªùi, kh√¥ng m·ªü file tr∆∞·ªõc
        with tempfile.TemporaryDirectory(dir=os.environ['TMPDIR']) as temp_dir:
            
            # 1. T·∫£i Label Encoder (.pkl)
            tmp_encoder_path = os.path.join(temp_dir, 'label_encoder.pkl')
            client.download('/label_encoder.pkl', tmp_encoder_path)
            LABEL_ENCODER = joblib.load(tmp_encoder_path)
            
            # 2. T·∫£i M√¥ h√¨nh Binary Classification (Delay/No Delay)
            tmp_delay_model_path = os.path.join(temp_dir, 'flight_delay_model.json')
            client.download('/flight_delay_model.json', tmp_delay_model_path)
            DELAY_MODEL = xgb.Booster()
            DELAY_MODEL.load_model(tmp_delay_model_path)

            # 3. T·∫£i M√¥ h√¨nh Multi-class Classification (Delay Cause)
            tmp_cause_model_path = os.path.join(temp_dir, 'flight_cause_model.json')
            client.download('/flight_cause_model.json', tmp_cause_model_path)
            CAUSE_MODEL = xgb.Booster()
            CAUSE_MODEL.load_model(tmp_cause_model_path)
        
        MODEL_LOAD_SUCCESS = True
        print("--- T·∫£i M√¥ h√¨nh D·ª± ƒëo√°n th√†nh c√¥ng ---")

    except HdfsError as e:
        print(f"--- L·ªñI HDFS: {e} ---")
        print("Vui l√≤ng ƒë·∫£m b·∫£o c√°c file '.pkl' v√† '.json' ƒë√£ ƒë∆∞·ª£c upload v√†o th∆∞ m·ª•c g·ªëc / tr√™n HDFS.")
        MODEL_LOAD_SUCCESS = False

    except Exception as e:
        print(f"--- L·ªñI T·∫¢I M√î H√åNH: {type(e).__name__}: {e} ---")
        traceback.print_exc()
        MODEL_LOAD_SUCCESS = False

def get_spark_session(hdfs_rpc_url):
    """T·∫°o ho·∫∑c tr·∫£ v·ªÅ Spark Session ƒë√£ t·ªìn t·∫°i."""
    global global_spark_session
    if global_spark_session is None:
        try:
            global_spark_session = SparkSession.builder \
                .appName("HDFSWebAppAnalysis") \
                .master("local[*]") \
                .config("spark.hadoop.fs.defaultFS", hdfs_rpc_url) \
                .config("spark.driver.extraJavaOptions", "--add-opens=java.base/java.lang=ALL-UNNAMED") \
                .config("spark.executor.extraJavaOptions", "--add-opens=java.base/java.lang=ALL-UNNAMED") \
                .config("spark.driver.maxResultSize", "4g") \
                .config("spark.local.dir", os.environ['TMPDIR']) \
                .config("spark.hadoop.hadoop.tmp.dir", os.environ['TMPDIR']) \
                .getOrCreate()
            print("--- Spark Session ƒë√£ kh·ªüi t·∫°o th√†nh c√¥ng ---")
        except Exception as e:
            print(f"--- L·ªñI KH·ªûI T·∫†O SPARK: {type(e).__name__}: {e} ---")
            traceback.print_exc()
            return None
    return global_spark_session


# --- CH·ª®C NƒÇNG T·∫†O BI·ªÇU ƒê·ªí ---

def generate_plot(top_data, file_path, title, x_label, y_label, data_key, label_key, horizontal=True):
    """T·∫°o bi·ªÉu ƒë·ªì chung cho c√°c ph√¢n t√≠ch ƒë·ªô tr·ªÖ/t·ª∑ l·ªá."""
    labels = [row[label_key] for row in top_data]
    values = [row[data_key] for row in top_data]

    fig, ax = plt.subplots(figsize=(10, 6))
    bars = ax.barh(labels, values, color='#ff7f0e') if horizontal else ax.bar(labels, values, color='#ff7f0e') 
    
    for bar in bars:
        if horizontal:
            ax.text(bar.get_width(), bar.get_y() + bar.get_height()/2, f'{bar.get_width():.2f}', va='center')
        else:
            ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{bar.get_height():.2f}', ha='center', va='bottom')
            
    ax.set_xlabel(x_label)
    ax.set_title(f'{title} trong {file_path}', fontsize=14)
    if horizontal: ax.invert_yaxis()
    plt.tight_layout()

    buf = BytesIO()
    plt.savefig(buf, format='png')
    plt.close(fig)
    
    data = base64.b64encode(buf.getbuffer()).decode("ascii")
    return f"data:image/png;base64,{data}"

def generate_word_count_plot(top_data, file_path):
    """T·∫°o bi·ªÉu ƒë·ªì Word Count v√† tr·∫£ v·ªÅ Base64 String."""
    words = [row['word'] for row in top_data]
    counts = [row['count'] for row in top_data]

    fig, ax = plt.subplots(figsize=(10, 6))
    ax.barh(words, counts, color='#007bff')
    
    ax.set_xlabel('T·∫ßn su·∫•t (Count)')
    ax.set_title(f'Top 10 T·ª´ Ph·ªï Bi·∫øn trong {file_path}', fontsize=14)
    ax.invert_yaxis()
    
    plt.tight_layout()

    buf = BytesIO()
    plt.savefig(buf, format='png')
    plt.close(fig)
    
    data = base64.b64encode(buf.getbuffer()).decode("ascii")
    return f"data:image/png;base64,{data}"

# H√ÄM T·∫†O BI·ªÇU ƒê·ªí FEATURE IMPORTANCE
def generate_feature_importance_plot(model, title):
    """Tr√≠ch xu·∫•t v√† tr·ª±c quan h√≥a Feature Importance t·ª´ m√¥ h√¨nh XGBoost Booster."""
    try:
        # L·∫•y importance (weight) t·ª´ Booster
        importance = model.get_score(importance_type='weight') if hasattr(model, 'get_score') else {}

        # Chuy·ªÉn sang danh s√°ch dictionary
        plot_data_list = []
        for feature, score in importance.items():
            plot_data_list.append({'FeatureName': feature, 'Score': float(score)})

        # N·∫øu r·ªóng, th·ª≠ d√πng attribute feature_names n·∫øu c√≥
        if not plot_data_list and hasattr(model, 'feature_names') and model.feature_names:
            plot_data_list = [{'FeatureName': name, 'Score': 0.0} for name in model.feature_names]

        # S·∫Øp x·∫øp gi·∫£m d·∫ßn
        plot_data_list.sort(key=lambda x: x['Score'], reverse=True)

        # Mapping t√™n ƒë·∫∑c tr∆∞ng sang ti·∫øng Vi·ªát (tu·ª≥ bi·∫øn)
        feature_name_map = {
            'Month': 'Th√°ng',
            'DayofMonth': 'Ng√†y trong th√°ng',
            'DayOfWeek': 'Th·ª© trong tu·∫ßn',
            'TaxiOut': 'Th·ªùi gian lƒÉn (TaxiOut)',
            'Distance': 'Kho·∫£ng c√°ch bay'
        }

        labels = [row['FeatureName'] for row in plot_data_list]
        values = [row['Score'] for row in plot_data_list]
        display_labels = [feature_name_map.get(l, l) for l in labels]

        # C·∫•u h√¨nh bi·ªÉu ƒë·ªì (k√≠ch th∆∞·ªõc ƒë·ªông theo s·ªë feature)
        plt.rcParams.update({'font.size': 11})
        height = max(4, 0.6 * len(values))
        fig, ax = plt.subplots(figsize=(12, height))

        # M√†u gradient n·∫øu c√≥ nhi·ªÅu bar
        if values:
            colors = plt.cm.RdYlBu_r(np.linspace(0.2, 0.8, len(values)))
        else:
            colors = ['#1f77b4']

        bars = ax.barh(display_labels, values, color=colors)

        # Th√™m nh√£n gi√° tr·ªã ·ªü cu·ªëi m·ªói thanh
        max_score = max(values) if values else 1
        for bar in bars:
            width = bar.get_width()
            pct = (width / max_score) * 100 if max_score > 0 else 0
            ax.text(width + max_score * 0.01, bar.get_y() + bar.get_height() / 2,
                    f"{width:.3f} ({pct:.1f}%)",
                    va='center', ha='left', fontsize=10, fontweight='bold')

        ax.set_xlabel('ƒêi·ªÉm t·∫ßm quan tr·ªçng (weight / F-score)', fontsize=12, fontweight='bold')
        ax.set_title(f"{title}\nƒê·ªô quan tr·ªçng c·ªßa c√°c ƒë·∫∑c tr∆∞ng", fontsize=14, fontweight='bold')
        ax.grid(True, axis='x', linestyle='--', alpha=0.6)
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        ax.invert_yaxis()
        plt.tight_layout()

        buf = BytesIO()
        plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')
        plt.close(fig)

        data = base64.b64encode(buf.getbuffer()).decode('ascii')
        return f"data:image/png;base64,{data}", plot_data_list

    except Exception as e:
        print(f"L·ªñI T·∫†O BI·ªÇU ƒê·ªí FEATURE IMPORTANCE: {e}")
        traceback.print_exc()
        return None, None


# --- ROUTES CHO QU·∫¢N L√ù L∆ØU TR·ªÆ (Gi·ªØ nguy√™n) ---

@app.route('/')
@app.route('/browse')
@app.route('/browse/<path:path>')
def browse(path='/'):
    """Hi·ªÉn th·ªã danh s√°ch file/th∆∞ m·ª•c trong HDFS"""
    try:
        full_path = '/' + path.lstrip('/')
        files = client.list(full_path, status=True)

        entries = []
        if full_path != '/':
            parent = os.path.dirname(full_path.rstrip('/'))
            if parent == '': parent = '/'
            back_path = '' if parent == '/' else parent
            entries.append({'name': '...', 'type': 'directory', 'path': back_path})

        for f, meta in files:
            entries.append({
                'name': f,
                'type': meta['type'].lower(),
                'path': os.path.join(full_path, f).replace('\\', '/')
            })

        breadcrumbs = full_path.strip('/').split('/')
        crumbs = []
        cur = ''
        for b in breadcrumbs:
            cur += '/' + b
            crumbs.append({'name': b or 'root', 'path': cur})
        
        entries.sort(key=lambda x: (x['type'] != 'directory', x['name']))

        return render_template('storage.html', entries=entries, current_path=full_path, breadcrumbs=crumbs)
    except Exception as e:
        return f"L·ªói khi duy·ªát HDFS: {e}"

@app.route('/upload', methods=['POST'])
def upload():
    try:
        # L·∫•y danh s√°ch file (c√≥ th·ªÉ nhi·ªÅu)
        files = request.files.getlist('file')
        path = request.form.get('path', '/')
        
        if not files or len(files) == 0:
            return jsonify({'message': 'Kh√¥ng c√≥ file n√†o ƒë∆∞·ª£c ch·ªçn ƒë·ªÉ upload'}), 400
        
        uploaded_files = []
        
        for file in files:
            hdfs_path = ('/' + os.path.join(path, file.filename).lstrip('/')).replace('\\', '/')
            client.write(hdfs_path, file, overwrite=True)
            uploaded_files.append(hdfs_path)
        
        return jsonify({
            'message': f'Upload th√†nh c√¥ng {len(uploaded_files)} file',
            'uploaded_files': uploaded_files
        })
        
    except Exception as e:
        traceback.print_exc()
        return jsonify({
            'message': f"L·ªói khi upload: {type(e).__name__}: {e}"
        }), 500


@app.route('/mkdir', methods=['POST'])
def make_directory():
    try:
        current_path = request.form.get('current_path', '/')
        new_dir_name = request.form.get('dir_name')
        if not new_dir_name:
            return jsonify({'message': 'T√™n th∆∞ m·ª•c kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng'}), 400
        hdfs_path = os.path.join(current_path, new_dir_name).replace('\\', '/')
        client.makedirs(hdfs_path) 
        return jsonify({'message': f'T·∫°o th∆∞ m·ª•c th√†nh c√¥ng: {hdfs_path}'})
    except Exception as e:
        traceback.print_exc()
        return jsonify({'message': f"L·ªói t·∫°o th∆∞ m·ª•c HDFS: {type(e).__name__}: {e}"}), 500

@app.route('/download/<path:path>')
def download(path):
    try:
        full_path = '/' + path.lstrip('/')
        with client.read(full_path) as reader:
            data = BytesIO(reader.read())
        filename = os.path.basename(path)
        return send_file(data, as_attachment=True, download_name=filename)
    except Exception as e:
        traceback.print_exc()
        return f"L·ªói t·∫£i file: {e}"

@app.route('/download_zip/<path:path>')
def download_zip(path):
    full_path = '/' + path.lstrip('/')
    dir_name = os.path.basename(full_path)
    zip_filename = f"{dir_name}.zip"

    with tempfile.TemporaryFile() as temp_zip_file:
        with zipfile.ZipFile(temp_zip_file, 'w', zipfile.ZIP_DEFLATED) as zipf:
            try:
                def zip_dir(hdfs_path, current_dir_name):
                    for name, status in client.list(hdfs_path, status=True):
                        item_path = os.path.join(hdfs_path, name).replace('\\', '/')
                        arcname = os.path.join(current_dir_name, name).replace('\\', '/')

                        if status['type'] == 'FILE':
                            with client.read(item_path) as reader:
                                data = reader.read()
                            zipf.writestr(arcname, data)
                        elif status['type'] == 'DIRECTORY':
                            zip_dir(item_path, arcname)

                zip_dir(full_path, dir_name)

            except Exception as e:
                traceback.print_exc()
                return f"L·ªói khi n√©n th∆∞ m·ª•c HDFS: {e}", 500

        temp_zip_file.seek(0)
        
        return send_file(
            BytesIO(temp_zip_file.read()),
            mimetype='application/zip',
            as_attachment=True,
            download_name=zip_filename
        )

@app.route('/delete/<path:path>', methods=['DELETE'])
def delete_file(path):
    try:
        hdfs_path = '/' + path.lstrip('/')
        client.delete(hdfs_path, recursive=True)
        return jsonify({'message': f'X√≥a th√†nh c√¥ng: {hdfs_path}'})
    except HdfsError as e:
        traceback.print_exc()
        return jsonify({'message': f'L·ªói HDFS khi x√≥a: {e}'}), 500
    except Exception as e:
        traceback.print_exc()
        return jsonify({'message': f'L·ªói kh√¥ng x√°c ƒë·ªãnh khi x√≥a: {e}'}), 500

# --- ROUTES CHO X·ª¨ L√ù D·ªÆ LI·ªÜU (ANALYSIS) ---

@app.route('/analysis')
def analysis_form():
    try:
        # files_for_analysis l√† list of tuples: (name, metadata)
        files_for_analysis = [f for f in client.list('/', status=True) if f[1]['type'] == 'FILE']
        return render_template('analysis.html', files=files_for_analysis) 
    except Exception as e:
        return f"L·ªói khi truy c·∫≠p HDFS cho ph√¢n t√≠ch: {e}"

# --- H√ÄM D·ª∞ ƒêO√ÅN (C·∫¶N PH·∫¢I C√ì) ---
def perform_prediction(row, result_lines):
    # ƒê·∫£m b·∫£o c√°c m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng
    global DELAY_MODEL, CAUSE_MODEL, LABEL_ENCODER
    
    # C√ÅC C·ªòT ƒê·∫¶U V√ÄO C·ª¶A M√î H√åNH D·ª∞ ƒêO√ÅN
    feature_cols = ["Month", "DayofMonth", "DayOfWeek", "TaxiOut", "Distance"]
    
    # 1. Chu·∫©n b·ªã Feature Vector (s·ª≠ d·ª•ng c√°c c·ªôt ƒë√£ l√†m s·∫°ch)
    # Ki·ªÉm tra xem row l√† Series hay DataFrame
    if isinstance(row, pd.Series):
        features_dict = row[feature_cols].apply(lambda x: x if pd.notna(x) else None).to_dict()
        df_row = row.to_frame().T
    elif isinstance(row, pd.DataFrame):
        features_dict = row[feature_cols].iloc[0].apply(lambda x: x if pd.notna(x) else None).to_dict()
        df_row = row
    else:
        result_lines.append("L·ªói: D·ªØ li·ªáu ƒë·∫ßu v√†o cho d·ª± ƒëo√°n kh√¥ng ƒë√∫ng ƒë·ªãnh d·∫°ng.")
        return None

    # Chuy·ªÉn ƒë·ªïi th√†nh DMatrix cho XGBoost
    try:
        data_matrix = xgb.DMatrix(df_row[feature_cols].values, feature_names=feature_cols)
    except Exception as e:
        result_lines.append(f"L·ªói t·∫°o DMatrix: {e}")
        return None
        
    # 2. D·ª± ƒëo√°n Delay/No Delay (Binary Classification)
    delay_proba = DELAY_MODEL.predict(data_matrix)[0] # X√°c su·∫•t tr·ªÖ (label 1)
    
    delay_prediction_status = "KH√îNG B·ªä TR·ªÑ"
    if delay_proba >= 0.5:
        delay_prediction_status = "C√ì KH·∫¢ NƒÇNG B·ªä TR·ªÑ"
        
    delay_probability_text = f"{delay_proba*100:.2f}%"

    # 3. D·ª± ƒëo√°n Nguy√™n nh√¢n Tr·ªÖ (Multi-class Classification)
    predicted_cause = "Kh√¥ng √°p d·ª•ng"
    if delay_prediction_status == "C√ì KH·∫¢ NƒÇNG B·ªä TR·ªÑ":
        cause_prediction_raw = CAUSE_MODEL.predict(data_matrix)
        predicted_cause_index = cause_prediction_raw.argmax()
        
        # Gi·∫£i m√£ nh√£n
        try:
            predicted_cause_encoded = LABEL_ENCODER.classes_[predicted_cause_index]
            cause_classes_map = {
                'CarrierDelay': 'Do H√£ng h√†ng kh√¥ng',
                'LateAircraftDelay': 'Do M√°y bay ƒë·∫øn tr·ªÖ',
                'NASDelay': 'Do H·ªá th·ªëng kh√¥ng l∆∞u (NAS)',
                'SecurityDelay': 'Do An ninh',
                'WeatherDelay': 'Do Th·ªùi ti·∫øt',
            }
            predicted_cause = cause_classes_map.get(predicted_cause_encoded, predicted_cause_encoded)

        except Exception as e:
            result_lines.append(f"L·ªói gi·∫£i m√£ nh√£n nguy√™n nh√¢n: {e}")
            predicted_cause = f"L·ªói gi·∫£i m√£ (Index: {predicted_cause_index})"
    else:
        predicted_cause = "Kh√¥ng c√≥ d·ª± ƒëo√°n nguy√™n nh√¢n do m√¥ h√¨nh cho r·∫±ng chuy·∫øn bay kh√¥ng tr·ªÖ"

    # 4. Tr√≠ch xu·∫•t Th·ª±c t·∫ø (Actuals)
    # L·∫•y c√°c c·ªôt th·ª±c t·∫ø t·ª´ file m·ªõi
    actual_arr_delay = row.get('ArrDelay')
    
    actual_status = "ƒê√öNG GI·ªú"
    # ƒêi·ªÅu ki·ªán tr·ªÖ th·ª±c t·∫ø: ArrDelay > 15 ph√∫t
    actual_is_delayed = actual_arr_delay is not None and pd.notna(actual_arr_delay) and actual_arr_delay > 15
    
    if actual_is_delayed:
        actual_status = f"TR·ªÑ TH·ª∞C T·∫æ (> 15 ph√∫t)"
    elif actual_arr_delay is not None and pd.notna(actual_arr_delay) and actual_arr_delay > 0:
        actual_status = f"TR·ªÑ NH·∫∏ TH·ª∞C T·∫æ (0 < ArrDelay ‚â§ 15)"

        
    # Tr√≠ch xu·∫•t chi ti·∫øt nguy√™n nh√¢n th·ª±c t·∫ø
    actual_cause_detail = "Kh√¥ng √°p d·ª•ng"
    if actual_is_delayed:
        delay_causes = {
            "CarrierDelay": row.get('CarrierDelay', 0.0),
            "WeatherDelay": row.get('WeatherDelay', 0.0),
            "NASDelay": row.get('NASDelay', 0.0),
            "SecurityDelay": row.get('SecurityDelay', 0.0),
            "LateAircraftDelay": row.get('LateAircraftDelay', 0.0)
        }
        
        # Ch·ªâ x√©t c√°c nguy√™n nh√¢n > 0
        actual_causes_list = [f"<b>{k}</b>: {v:.0f}p" for k, v in delay_causes.items() if v is not None and v > 0]
        actual_cause_detail = "<br>".join(actual_causes_list) or "Kh√¥ng c√≥ nguy√™n nh√¢n chi ti·∫øt ƒë∆∞·ª£c ghi nh·∫≠n (T·∫•t c·∫£ ƒë·ªÅu 0)"
    
    # 5. So s√°nh D·ª± ƒëo√°n v√† Th·ª±c t·∫ø
    match_status = "Kh√¥ng th·ªÉ x√°c ƒë·ªãnh"
    if actual_arr_delay is not None and pd.notna(actual_arr_delay):
        # Tr√πng kh·ªõp n·∫øu c·∫£ hai c√πng d·ª± ƒëo√°n l√† tr·ªÖ (>15p) ho·∫∑c c√πng d·ª± ƒëo√°n kh√¥ng tr·ªÖ (<=15p)
        if (delay_prediction_status == "C√ì KH·∫¢ NƒÇNG B·ªä TR·ªÑ" and actual_is_delayed) or \
           (delay_prediction_status == "KH√îNG B·ªä TR·ªÑ" and not actual_is_delayed):
             match_status = "‚úÖ TR√ôNG KH·ªöP"
        else:
             match_status = "‚ùå KH√îNG TR√ôNG KH·ªöP"
    else:
        actual_status = "KH√îNG C√ì ArrDelay G·ªêC"


    # 6. Tr·∫£ v·ªÅ k·∫øt qu·∫£
    return {
        'delay_prediction': delay_prediction_status,
        'delay_probability': delay_probability_text,
        'cause_prediction': predicted_cause,
        'actual_status': actual_status,
        'actual_arr_delay': f"{actual_arr_delay:.2f}p" if actual_arr_delay is not None and pd.notna(actual_arr_delay) else "N/A",
        'actual_cause_detail': actual_cause_detail,
        'match_status': match_status,
        'processed_features': "\n".join([f"{k}: {v}" for k, v in features_dict.items() if v is not None])
    }
    
@app.route('/run_analysis', methods=['POST'])
def run_analysis():
    """Th·ª±c hi·ªán m·ªôt t√°c v·ª• ph√¢n t√≠ch ph√¢n t√°n b·∫±ng Apache Spark."""
    file_path = request.form.get('file_path')
    analysis_type = request.form.get('analysis_type')

    spark_session = get_spark_session(HDFS_RPC_URL) 
    
    if spark_session is None:
        return render_template('result.html', lines=["L·ªñI: Kh√¥ng th·ªÉ kh·ªüi t·∫°o Spark Session. Vui l√≤ng ki·ªÉm tra c√†i ƒë·∫∑t PySpark, JAVA_HOME, v√† SPARK_HOME."], analysis_type=analysis_type), 500

    if not file_path:
        return jsonify({'message': 'Vui l√≤ng ch·ªçn file ƒë·ªÉ ph√¢n t√≠ch'}), 400
        
    result_lines = []
    plot_base64 = None 
    prediction_data = None 
    hdfs_full_path = file_path 

    # --- ƒê·ªäNH NGHƒ®A C√ÅC LO·∫†I PH√ÇN T√çCH CSV ---
    csv_analysis_types = [
        'avg_delay_by_origin', 
        'percentage_delayed_flights', 
        'delay_causes_breakdown', 
        'delay_prediction', 
        'feature_importance_analysis'
    ]

    try:
        if analysis_type in csv_analysis_types:
            
            # ƒê·ªçc CSV 
            raw_df = spark_session.read.csv(hdfs_full_path, header=True)
            
            # --- L√ÄM S·∫†CH V√Ä √âP KI·ªÇU T·∫§T C·∫¢ C√ÅC C·ªòT C·∫¶N THI·∫æT ---
            
            # C√ÅC C·ªòT C·∫¶N THI·∫æT T·ª™ C·∫§U TR√öC M·ªöI (S·ªë h·ªçc)
            delay_cols = ["ArrDelay", "DepDelay", "CarrierDelay", "WeatherDelay", "NASDelay", "SecurityDelay", "LateAircraftDelay"]
            # C√ÅC C·ªòT D·ª∞ ƒêO√ÅN T·ª™ C·∫§U TR√öC M·ªöI (S·ªë h·ªçc)
            prediction_cols = ["Month", "DayofMonth", "DayOfWeek", "TaxiOut", "Distance"]
            # C·ªôt Origin ƒë·ªÉ ph√¢n t√≠ch theo s√¢n bay (Chu·ªói)
            analysis_cols = ["Origin"]
            
            all_needed_cols = delay_cols + prediction_cols + analysis_cols
            
            df = raw_df
            
            # L·∫∑p qua c√°c c·ªôt s·ªë v√† √©p ki·ªÉu/l√†m s·∫°ch
            columns_to_clean = delay_cols + prediction_cols
            for col_name in columns_to_clean:
                if col_name in df.columns:
                    # Ghi ƒë√® c·ªôt g·ªëc b·∫±ng c·ªôt ƒë√£ l√†m s·∫°ch v√† √©p ki·ªÉu Double
                    df = df.withColumn(
                        col_name,
                        regexp_replace(col(col_name).cast(StringType()), "[^\\d\\.\\-]", "").cast(DoubleType())
                    )
                else:
                    result_lines.append(f"C·∫¢NH B√ÅO: Kh√¥ng t√¨m th·∫•y c·ªôt '{col_name}' trong file. Ph√¢n t√≠ch/D·ª± ƒëo√°n b·ªã ·∫£nh h∆∞·ªüng.")
            
            # L·∫•y d√≤ng ƒë·∫ßu ti√™n ƒë·ªÉ hi·ªÉn th·ªã log
            with client.read(hdfs_full_path, encoding='utf-8') as reader:
                try:
                    all_lines = reader.readlines()
                    original_raw_line = all_lines[1].strip() if len(all_lines) > 1 else "Kh√¥ng th·ªÉ ƒë·ªçc d√≤ng d·ªØ li·ªáu g·ªëc ƒë·∫ßu ti√™n."
                except:
                    original_raw_line = "Kh√¥ng th·ªÉ ƒë·ªçc d√≤ng d·ªØ li·ªáu g·ªëc ƒë·∫ßu ti√™n."
            
            # DataFrame s·∫°ch ch·ªâ gi·ªØ l·∫°i c√°c c·ªôt c·∫ßn thi·∫øt sau khi ƒë√£ √©p ki·ªÉu
            df_clean = df.select(*all_needed_cols).dropna(subset=prediction_cols + ["ArrDelay"])
            
            # ----------------------------------------------------
            # B·∫ÆT ƒê·∫¶U CHU·ªñI IF/ELIF CHO C√ÅC LO·∫†I PH√ÇN T√çCH CSV KH√ÅC NHAU
            # ----------------------------------------------------

            # --- LOGIC CHO feature_importance_analysis (ƒê√É S·ª¨A L·ªñI C√ö PH√ÅP: elif -> if) ---
            if analysis_type == 'feature_importance_analysis':
                if not MODEL_LOAD_SUCCESS:
                    result_lines.append("L·ªñI: Kh√¥ng th·ªÉ t·∫£i m√¥ h√¨nh (DELAY_MODEL ho·∫∑c CAUSE_MODEL). Vui l√≤ng ki·ªÉm tra file tr√™n HDFS.")
                    plot_base64 = None
                else:
                    result_lines.append("--- B∆Ø·ªöC 1: PH√ÇN T√çCH T·∫¶M QUAN TR·ªåNG C·ª¶A ƒê·∫∂C TR∆ØNG ---")
                    
                    # 1. PH√ÇN T√çCH T·∫¶M QUAN TR·ªåNG C·ª¶A ƒê·∫∂C TR∆ØNG (V·∫Ω 2 bi·ªÉu ƒë·ªì Features)
                    delay_plot_base64, delay_data = generate_feature_importance_plot(
                        DELAY_MODEL, 
                        "T·∫ßm quan tr·ªçng ƒê·∫∑c tr∆∞ng - D·ª± ƒëo√°n ƒê·ªô tr·ªÖ (Delay/No Delay)"
                    )
                    cause_plot_base64, cause_data = generate_feature_importance_plot(
                        CAUSE_MODEL, 
                        "T·∫ßm quan tr·ªçng ƒê·∫∑c tr∆∞ng - D·ª± ƒëo√°n Nguy√™n nh√¢n Tr·ªÖ (Multi-Class)"
                    )
                    
                    # L∆∞u 2 bi·ªÉu ƒë·ªì Feature Importance v√†o log ƒë·ªÉ hi·ªÉn th·ªã chi ti·∫øt
                    result_lines.append("\nChi ti·∫øt Feature Importance Model Delay:")
                    for item in delay_data: result_lines.append(f"  - {item['FeatureName']}: {item['Score']:.4f}")
                    result_lines.append("\nChi ti·∫øt Feature Importance Model Cause:")
                    for item in cause_data: result_lines.append(f"  - {item['FeatureName']}: {item['Score']:.4f}")

                    
                    result_lines.append("\n--- B∆Ø·ªöC 2: √ÅP D·ª§NG M√î H√åNH V√Ä T√çNH T·ª∂ L·ªÜ TR√äN D·ªÆ LI·ªÜU N·ªÄN ---")

                    # 2. √ÅP D·ª§NG M√î H√åNH L√äN D·ªÆ LI·ªÜU S·∫†CH (df_clean)
                    
                    # Chuy·ªÉn ƒë·ªïi df_clean sang Pandas ƒë·ªÉ ch·∫°y d·ª± ƒëo√°n h√†ng lo·∫°t (hi·ªáu qu·∫£ h∆°n)
                    df_pd = df_clean.select(*df_clean.columns).toPandas()
                    
                    if df_pd.empty:
                        result_lines.append("L·ªñI D·ªÆ LI·ªÜU: Kh√¥ng c√≥ d·ªØ li·ªáu h·ª£p l·ªá ƒë·ªÉ √°p d·ª•ng m√¥ h√¨nh.")
                        plot_base64 = None
                    else:
                        feature_order = prediction_cols # S·ª≠ d·ª•ng c√°c c·ªôt d·ª± ƒëo√°n m·ªõi
                        input_matrix = xgb.DMatrix(df_pd[feature_order].values, feature_names=feature_order)
                        
                        # D·ª± ƒëo√°n Delay/No Delay (X√°c su·∫•t tr·ªÖ)
                        delay_probs = DELAY_MODEL.predict(input_matrix)
                        df_pd['PredictedDelay'] = delay_probs
                        df_pd['IsDelayed'] = (df_pd['PredictedDelay'] >= 0.5).astype(int) # 1: Delayed, 0: On-time
                        
                        # D·ª± ƒëo√°n Cause 
                        cause_preds_raw = CAUSE_MODEL.predict(input_matrix)
                        predicted_cause_indices = cause_preds_raw.argmax(axis=1)
                        
                        # Gi·∫£i m√£ nh√£n (LabelEncoder)
                        predicted_causes_encoded = LABEL_ENCODER.inverse_transform(predicted_cause_indices)
                        
                        # Map nh√£n nguy√™n nh√¢n
                        cause_classes_map = {
                            'CarrierDelay': 'H√£ng h√†ng kh√¥ng',
                            'LateAircraftDelay': 'M√°y bay ƒë·∫øn tr·ªÖ',
                            'NASDelay': 'H·ªá th·ªëng kh√¥ng l∆∞u (NAS)',
                            'SecurityDelay': 'An ninh',
                            'WeatherDelay': 'Th·ªùi ti·∫øt',
                        }
                        
                        df_pd['PredictedCause'] = [
                            cause_classes_map.get(predicted_causes_encoded[i], 'Unknown') 
                            if df_pd.loc[i, 'IsDelayed'] == 1 else 'Not Delayed' 
                            for i in range(len(df_pd))
                        ]
                        
                        # 3. TH·ªêNG K√ä V√Ä T·∫†O BI·ªÇU ƒê·ªí K·∫æT QU·∫¢
                        
                        # 3a. T·ª∑ l·ªá Delay
                        total_records = len(df_pd)
                        predicted_delay_count = df_pd['IsDelayed'].sum()
                        predicted_delay_percentage = (predicted_delay_count / total_records) * 100
                        
                        delay_stats = [
                            {'Status': 'Delayed', 'Percentage': predicted_delay_percentage},
                            {'Status': 'On-time', 'Percentage': 100 - predicted_delay_percentage}
                        ]
                        
                        # Bi·ªÉu ƒë·ªì 3: T·ª∑ l·ªá Delayed/On-time
                        result_lines.append(f"\nT·ªïng s·ªë b·∫£n ghi ƒë√£ √°p d·ª•ng m√¥ h√¨nh: {total_records}")
                        result_lines.append(f"T·ª∑ l·ªá D·ª± ƒëo√°n B·ªã tr·ªÖ: {predicted_delay_percentage:.2f}%")
                        delay_rate_plot = generate_plot(delay_stats, file_path, "T·ª∑ l·ªá D·ª± ƒëo√°n B·ªã Tr·ªÖ/ƒê√∫ng gi·ªù", "T·ª∑ l·ªá (%)", "Tr·∫°ng th√°i", "Percentage", "Status", horizontal=False) # V·∫Ω c·ªôt d·ªçc
                        
                        # 3b. T·ª∑ tr·ªçng Nguy√™n nh√¢n Delay
                        cause_counts = df_pd[df_pd['IsDelayed'] == 1]['PredictedCause'].value_counts()
                        
                        cause_breakdown = []
                        for cause, count in cause_counts.items():
                            percentage = (count / predicted_delay_count) * 100 if predicted_delay_count > 0 else 0
                            cause_breakdown.append({'Cause': cause, 'Percentage': percentage})
                            
                        # Bi·ªÉu ƒë·ªì 4: T·ª∑ tr·ªçng Nguy√™n nh√¢n (ch·ªâ hi·ªÉn th·ªã n·∫øu c√≥ tr·ªÖ)
                        if predicted_delay_count > 0:
                            cause_breakdown_plot = generate_plot(cause_breakdown, file_path, "T·ª∑ tr·ªçng Nguy√™n nh√¢n ƒê·ªô tr·ªÖ D·ª± ƒëo√°n", "T·ª∑ l·ªá (%)", "Nguy√™n nh√¢n", "Percentage", "Cause")
                            result_lines.append("\nChi ti·∫øt T·ª∑ tr·ªçng Nguy√™n nh√¢n D·ª± ƒëo√°n:")
                            for item in cause_breakdown: result_lines.append(f"  - {item['Cause']}: {item['Percentage']:.2f}%")
                        else:
                            cause_breakdown_plot = None
                            result_lines.append("\nKh√¥ng c√≥ b·∫£n ghi n√†o ƒë∆∞·ª£c d·ª± ƒëo√°n l√† b·ªã tr·ªÖ n√™n kh√¥ng ph√¢n t√≠ch nguy√™n nh√¢n.")


                        # G·ªôp t·∫•t c·∫£ 4 bi·ªÉu ƒë·ªì v√†o dictionary ƒë·ªÉ g·ª≠i v·ªÅ result.html
                        plot_base64 = {
                            'delay_rate_plot': delay_rate_plot,
                            'delay_cause_plot': cause_breakdown_plot,
                            'feature_delay': delay_plot_base64,
                            'feature_cause': cause_plot_base64
                        }
            
            # --- LOGIC CHO delay_prediction ---
            elif analysis_type == 'delay_prediction':
                if not MODEL_LOAD_SUCCESS:
                    result_lines.append("L·ªñI: Kh√¥ng th·ªÉ t·∫£i m√¥ h√¨nh. Ch·ª©c nƒÉng d·ª± ƒëo√°n kh√¥ng th·ªÉ th·ª±c hi·ªán.")
                else:
                    result_lines.append("ƒêang th·ª±c hi·ªán D·ª± ƒëo√°n ƒê·ªô tr·ªÖ v√† Nguy√™n nh√¢n cho b·∫£n ghi ƒë·∫ßu ti√™n...")
                    
                    # L·∫•y d√≤ng ƒë·∫ßu ti√™n h·ª£p l·ªá v√† chuy·ªÉn sang Pandas
                    first_row_pd = df_clean.limit(1).toPandas()
                    
                    if not first_row_pd.empty:
                        # G·ªçi h√†m helper
                        prediction_data = perform_prediction(first_row_pd.iloc[0], result_lines)
                        prediction_data['original_raw_line'] = original_raw_line
                    else:
                        result_lines.append("Kh√¥ng c√≥ d·ªØ li·ªáu h·ª£p l·ªá trong file ƒë·ªÉ d·ª± ƒëo√°n.")


            # --- C√ÅC PH√ÇN T√çCH TH·ªêNG K√ä C√íN L·∫†I (ƒê√£ c·∫≠p nh·∫≠t ƒë·ªÉ d√πng c·ªôt m·ªõi) ---
            elif analysis_type == 'avg_delay_by_origin':
                result_df = df_clean.groupBy("Origin").agg(
                    spark_round(avg(col("ArrDelay")), 2).alias("AvgArrivalDelay")
                ).filter(col("AvgArrivalDelay").isNotNull()).orderBy(col("AvgArrivalDelay").desc())

                top_10 = result_df.limit(10).collect()
                result_lines = [f"{row['Origin']}: {row['AvgArrivalDelay']} ph√∫t" for row in top_10]
                plot_base64 = generate_plot(top_10, file_path, "ƒê·ªô Tr·ªÖ ƒê·∫øn Trung B√¨nh (Top 10)", "Th·ªùi gian tr·ªÖ trung b√¨nh (ph√∫t)", "S√¢n bay (Origin)", "AvgArrivalDelay", "Origin")

            elif analysis_type == 'percentage_delayed_flights':
                total_flights = df_clean.count()
                delay_threshold = 15.0
                # ArrDelay ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch v√† √©p ki·ªÉu n√™n d√πng tr·ª±c ti·∫øp
                delayed_flights_df = df_clean.withColumn("IsDelayed", when(col("ArrDelay") > delay_threshold, 1).otherwise(0))
                
                result_df = delayed_flights_df.groupBy("Origin").agg(
                    spark_round((spark_count(when(col("IsDelayed") == 1, 1)) / spark_count("*")) * 100, 2).alias("DelayPercentage")
                ).filter(col("DelayPercentage").isNotNull()).orderBy(col("DelayPercentage").desc())

                top_10 = result_df.limit(10).collect()
                result_lines = [f"{row['Origin']}: {row['DelayPercentage']}%" for row in top_10]
                plot_base64 = generate_plot(top_10, file_path, "T·ª∑ l·ªá Chuy·∫øn bay B·ªã Tr·ªÖ (Top 10)", "T·ª∑ l·ªá (%)", "S√¢n bay (Origin)", "DelayPercentage", "Origin")

            elif analysis_type == 'delay_causes_breakdown':
                delay_causes = ["CarrierDelay", "WeatherDelay", "NASDelay", "SecurityDelay", "LateAircraftDelay"]
                # C√°c c·ªôt nguy√™n nh√¢n ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch v√† √©p ki·ªÉu n√™n d√πng tr·ª±c ti·∫øp
                df_delayed = df_clean.filter(col("ArrDelay") > 0).fillna(0.0, subset=delay_causes)
                
                # T√≠nh t·ªïng t·ª´ng lo·∫°i nguy√™n nh√¢n
                agg_exprs = [sum(col(c)).alias(f"Sum_{c}") for c in delay_causes]
                total_delay_sum = df_delayed.agg(*agg_exprs).collect()[0]
                
                # T√≠nh t·ªïng trong Python sau khi ƒë√£ collect (d√πng built-in sum ‚Äî tr√°nh xung ƒë·ªôt v·ªõi pyspark.sql.functions.sum)
                total_sum = builtins.sum(float(total_delay_sum[f"Sum_{c}"]) for c in delay_causes)

                if total_sum > 0:
                    breakdown_data = []
                    for cause in delay_causes:
                        value = total_delay_sum[f"Sum_{cause}"]
                        percentage = (value / total_sum) * 100 if value is not None else 0
                        breakdown_data.append({'Cause': cause, 'Percentage': percentage})
                    
                    breakdown_data.sort(key=lambda x: x['Percentage'], reverse=True)
                    
                    result_lines = [f"{item['Cause']}: {item['Percentage']:.2f}%" for item in breakdown_data]
                    plot_base64 = generate_plot(breakdown_data, file_path, "T·ª∑ tr·ªçng Nguy√™n nh√¢n ƒê·ªô tr·ªÖ", "T·ª∑ l·ªá (%)", "Nguy√™n nh√¢n", "Percentage", "Cause")
                else:
                    result_lines.append("Kh√¥ng t√¨m th·∫•y t·ªïng ƒë·ªô tr·ªÖ d∆∞∆°ng ƒë·ªÉ ph√¢n t√≠ch nguy√™n nh√¢n.")
            
            # Kh·ªëi n√†y kh√¥ng c·∫ßn thi·∫øt v√¨ ƒë√£ c√≥ check ·ªü ƒë·∫ßu h√†m, nh∆∞ng gi·ªØ l·∫°i ƒë·ªÉ b·∫Øt l·ªói n·∫øu c√≥
            else:
                 raise ValueError("Lo·∫°i ph√¢n t√≠ch CSV kh√¥ng h·ª£p l·ªá ƒë√£ v∆∞·ª£t qua b·ªô l·ªçc ban ƒë·∫ßu.")

        # --- LOGIC CHO LINE COUNT & WORD COUNT (TEXT FILES) ---
        elif analysis_type == 'line_count':
            count = spark_session.sparkContext.textFile(hdfs_full_path).count()
            result_lines.append(f"File '{file_path}' c√≥ t·ªïng c·ªông {count} d√≤ng.")

        elif analysis_type == 'word_count':
            lines = spark_session.sparkContext.textFile(hdfs_full_path).take(1001) 
            if len(lines) > 1:
                lines = lines[1:]
            rdd = spark_session.sparkContext.parallelize(lines)
            
            word_counts = rdd.flatMap(lambda line: split(line, '\s+')) \
                            .filter(lambda word: word != "") \
                            .map(lambda word: (lower(word), 1)) \
                            .reduceByKey(lambda a, b: a + b) \
                            .map(lambda x: (x[1], x[0])) \
                            .sortByKey(False) \
                            .take(10)
                            
            top_10 = [{'word': word, 'count': count} for count, word in word_counts]
            result_lines = [f"{i+1}. {item['word']}: {item['count']}" for i, item in enumerate(top_10)]
            plot_base64 = generate_word_count_plot(top_10, file_path)

        else:
            return jsonify({'message': 'Lo·∫°i ph√¢n t√≠ch kh√¥ng h·ª£p l·ªá'}), 400

    except ValueError as ve:
        return jsonify({'message': f'{ve}'}), 400
        
    except HdfsError as e:
        result_lines.append(f"L·ªñI HDFS: Kh√¥ng th·ªÉ truy c·∫≠p file {hdfs_full_path}. L·ªói: {e}")
        traceback.print_exc()
        
    except Exception as e:
        result_lines.append(f"L·ªñ·ªñI PH√ÇN T√çCH: {type(e).__name__}: {e}")
        traceback.print_exc()

    return render_template('result.html', 
                            lines=result_lines, 
                            analysis_type=analysis_type, 
                            plot_base64=plot_base64, 
                            prediction_data=prediction_data)


if __name__ == '__main__':
    app.run(debug=True, port=5000)